```python
python -m unittest test_my_math.py

```

```python
pytest
```

```python
pytest -v test_reqres_api.py
```

Good one âš¡ï¸ Running tests in parallel can make your test suite **much faster**. With **pytest**, you do this using the [`pytest-xdist`](https://pypi.org/project/pytest-xdist/) plugin.

---

## ðŸ› ï¸ Setup

Install the plugin:

```bash
pip install pytest-xdist
```

---

## â–¶ï¸ Run all tests in parallel

### 1. Run with N processes (e.g., 4)

```bash
pytest -n 4
```

This will split tests across **4 workers**.

---

### 2. Run on all available CPU cores

```bash
pytest -n auto
```

Pytest will auto-detect the number of CPU cores and distribute tests across them.

---

### 3. Run with load balancing

```bash
pytest -n auto --dist=loadscope
```

* `--dist=loadscope` â†’ ensures tests from the same module/class run in the same worker (good if you have shared setup/teardown).
* `--dist=load` â†’ fully random distribution (better for performance, but riskier if tests depend on shared state).

---

## âš ï¸ Things to watch out for

* Parallel tests must be **independent** (no shared DB/file state unless isolated).
* If using temporary files, use `tmp_path` fixture to avoid clashes.
* For API tests, make sure test data (like `POST` â†’ `DELETE`) doesnâ€™t interfere across workers.

---

ðŸ‘‰ Do you want me to also show you how to **combine parallel execution with markers** (e.g., run only `api` tests in parallel while skipping slow ones)?

-------
Great question ðŸ™Œ â€” in `pytest` you can **focus on only specific tests** or **skip certain tests**. Hereâ€™s how you do both:

---

# âœ… Run Only Specific Tests

### 1. Using `-k` (keyword expression)

Run tests whose names match a keyword:

```bash
pytest -k "login"
```

ðŸ‘‰ Runs all tests with `"login"` in the name.

---

### 2. Run a specific test function

```bash
pytest test_file.py::test_function_name
```

Or inside a class:

```bash
pytest test_file.py::TestClass::test_method
```

---

### 3. Mark a test with `@pytest.mark.only` (custom trick)

`pytest` doesnâ€™t have built-in `.only` like JavaScriptâ€™s Jest/PactumJS, but you can simulate it:
refer: https://docs.pytest.org/en/stable/how-to/mark.html
```python
import pytest

@pytest.mark.only
def test_focus_this():
    assert 1 + 1 == 2

def test_other():
    assert 2 + 2 == 4
```

Run only tests with that mark:

```bash
pytest -m only
```

---

# ðŸš« Skip Tests

### 1. Skip using decorator

```python
import pytest

@pytest.mark.skip(reason="Feature not ready yet")
def test_future_feature():
    assert False
```

---

### 2. Skip conditionally

```python
import sys
import pytest

@pytest.mark.skipif(sys.platform == "win32", reason="Does not run on Windows")
def test_not_on_windows():
    assert True
```

---

### 3. Dynamically skip inside a test

```python
import pytest

def test_dynamic_skip():
    if True:  # some condition
        pytest.skip("Skipping due to unmet condition")
    assert 1 == 1
```

---

# âš¡ Pro tip

* Use `pytest -rs` to **see skip reasons** in the report.
* You can combine: run only some tests while skipping others.

---

ðŸ‘‰ Do you want me to also show you how to **mark tests as â€œslowâ€** and skip them by default (but run only when you explicitly ask for `--runslow`)?

Perfect question ðŸ™Œ â€” **fixtures are one of pytestâ€™s most powerful features**, and understanding them well makes your test suites **cleaner, faster, and reusable**. Letâ€™s break it down:

---

# ðŸ”¹ What is a Fixture in pytest?

A **fixture** is a function that provides test functions (or test classes) with some *setup resources*.

* Think of it as a **dependency injector**.
* Examples: database connection, API client, test data, temp files.
* Instead of writing setup code in every test, you define a fixture once, and pytest automatically passes it where needed.

---

# ðŸ”¹ Basic Example

```python
import pytest

@pytest.fixture
def sample_data():
    return {"name": "Alice", "age": 30}

def test_name(sample_data):
    assert sample_data["name"] == "Alice"

def test_age(sample_data):
    assert sample_data["age"] == 30
```

ðŸ‘‰ `sample_data` fixture is injected into both tests automatically.

---

# ðŸ”¹ Fixture Scopes

You can control **how often a fixture runs** with `scope`.

* `function` (default) â†’ runs fresh for each test.
* `class` â†’ runs once per class of tests.
* `module` â†’ runs once per test file.
* `session` â†’ runs once per entire test run.

Example:

```python
@pytest.fixture(scope="module")
def db_connection():
    print("\nSetting up DB")
    yield {"db": "connected"}
    print("\nTearing down DB")
```

* `yield` â†’ lets you define **setup** (before yield) and **teardown** (after yield).
* Runs only **once for the module**.

---

# ðŸ”¹ Using `autouse` Fixtures

If you donâ€™t want to **explicitly request** a fixture, you can make it run automatically:

```python
@pytest.fixture(autouse=True)
def setup_logging():
    print("\nLogging initialized")
```

ðŸ‘‰ This will run for **every test automatically**.

---

# ðŸ”¹ Fixtures in Classes

```python
class TestAPI:
    @pytest.fixture(autouse=True)
    def setup_client(self):
        self.client = {"name": "API Client"}

    def test_client_has_name(self):
        assert self.client["name"] == "API Client"
```

ðŸ‘‰ The `setup_client` fixture runs before each method in the class, initializing `self.client`.

---

# ðŸ”¹ Parametrized Fixtures

You can run the same test with multiple fixture values:

```python
@pytest.fixture(params=[1, 2, 3])
def number(request):
    return request.param

def test_number_is_positive(number):
    assert number > 0
```

ðŸ‘‰ Runs the test **3 times**: with `1`, `2`, and `3`.

---

# ðŸ”¹ Fixture Dependency (Fixtures Using Fixtures)

Fixtures can **depend on other fixtures**:

```python
@pytest.fixture
def user_id():
    return 42

@pytest.fixture
def user_profile(user_id):
    return {"id": user_id, "name": "Alice"}

def test_user_profile(user_profile):
    assert user_profile["id"] == 42
```

ðŸ‘‰ `user_profile` depends on `user_id`, and pytest resolves this automatically.

---

# ðŸ”¹ Example: API Client with Setup/Teardown

```python
import pytest
import requests

BASE_URL = "https://reqres.in/api"

@pytest.fixture(scope="session")
def api_client():
    session = requests.Session()
    yield session
    session.close()

def test_get_users(api_client):
    response = api_client.get(f"{BASE_URL}/users?page=2")
    assert response.status_code == 200
```

ðŸ‘‰ `api_client` is created once per session, reused in all tests, and cleaned up after.

---

# âœ… Summary of Fixtures

* **Keep tests DRY** (donâ€™t repeat setup code).
* **Scalable**: one fixture can power many tests.
* **Reusable**: can depend on other fixtures.
* **Flexible**: support parametrization, autouse, and different lifetimes (scopes).

---

ðŸ‘‰ Fixtures are the **#1 reason pytest is preferred over unittest** â€” they let you design **clean, modular, and maintainable** tests.

Would you like me to also create a **cheat sheet PDF with all pytest fixture patterns** (basic, scope, autouse, parametrization, dependency) for quick reference?

```bash
pytest --junitxml=reports/junit-report.xml
```

```bash
python split_junitxml.py reports/junit-report.xml reports/output.xml
```